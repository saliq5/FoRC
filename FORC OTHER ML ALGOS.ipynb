{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7577852,"sourceType":"datasetVersion","datasetId":4411470},{"sourceId":7678251,"sourceType":"datasetVersion","datasetId":4479306}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-22T12:24:52.984607Z","iopub.execute_input":"2024-02-22T12:24:52.984985Z","iopub.status.idle":"2024-02-22T12:24:53.355895Z","shell.execute_reply.started":"2024-02-22T12:24:52.984954Z","shell.execute_reply":"2024-02-22T12:24:53.355100Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/kaggle/train.csv')\ndata.drop(['doi','url','publication month', 'publication year','publisher', 'data_index'], axis =1 , inplace = True)\ndata = data.dropna()\ndata[\"text\"] = data[\"title\"] + data[\"abstract\"]\n\n\n\ndef NLP_cleaning(text):\n    text_corpus = []\n    i=0\n    for sent in tqdm(text, desc='Cleaning'):\n        # print(i, end =\" \")\n        i+=1\n        sent = re.sub('<[^>]*>', '', sent)\n        sent = re.sub('[^a-zA-z0-9]', ' ', sent)\n        sent = sent.lower()\n        text_corpus.append(sent)\n\n    return text_corpus\n\n\ntext = data.text.values.tolist()\ntext_corpus = NLP_cleaning(text)\ndata['text'] = text_corpus\ndata['title'] = NLP_cleaning(data.title.values.tolist())\ndata['author'] = NLP_cleaning(data.author.values.tolist())\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndata['label_number'] = label_encoder.fit_transform(data['label'])\n\n\nval_df = pd.read_csv('/kaggle/input/kaggle/val.csv')\nval_df.drop(['doi','url','publication month', 'publication year','publisher', 'data_index'], axis =1 , inplace = True)\nval_df = val_df.dropna()\nval_df['label_number'] = label_encoder.transform(val_df['label'])\nval_df[\"text\"] = val_df[\"title\"] + val_df[\"abstract\"]\nval_df['title'] = NLP_cleaning(val_df.title.values.tolist())\nval_df['author'] = NLP_cleaning(val_df.author.values.tolist())\nval_df['abstract'] = NLP_cleaning(val_df.abstract.values.tolist())\nval_df['text'] = NLP_cleaning(val_df.text.values.tolist())\n\n\n\n!pip install -q sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:24:53.357221Z","iopub.execute_input":"2024-02-22T12:24:53.357599Z","iopub.status.idle":"2024-02-22T12:25:17.383539Z","shell.execute_reply.started":"2024-02-22T12:24:53.357573Z","shell.execute_reply":"2024-02-22T12:25:17.382725Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Cleaning: 100%|██████████| 40332/40332 [00:03<00:00, 11936.94it/s]\nCleaning: 100%|██████████| 40332/40332 [00:00<00:00, 103696.38it/s]\nCleaning: 100%|██████████| 40332/40332 [00:00<00:00, 85508.26it/s]\nCleaning: 100%|██████████| 8648/8648 [00:00<00:00, 110582.36it/s]\nCleaning: 100%|██████████| 8648/8648 [00:00<00:00, 83774.48it/s]\nCleaning: 100%|██████████| 8648/8648 [00:00<00:00, 12358.51it/s]\nCleaning: 100%|██████████| 8648/8648 [00:00<00:00, 11908.37it/s]\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train = data['text']\nX_train = X_train.to_list()\nY_train = data['label_number']\nY_train = Y_train.to_list()","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:25:17.384720Z","iopub.execute_input":"2024-02-22T12:25:17.385195Z","iopub.status.idle":"2024-02-22T12:25:17.403022Z","shell.execute_reply.started":"2024-02-22T12:25:17.385168Z","shell.execute_reply":"2024-02-22T12:25:17.402162Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_test = val_df['text'].to_list()\nY_test = val_df['label_number'].to_list()","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:25:17.404864Z","iopub.execute_input":"2024-02-22T12:25:17.405138Z","iopub.status.idle":"2024-02-22T12:25:17.419144Z","shell.execute_reply.started":"2024-02-22T12:25:17.405114Z","shell.execute_reply":"2024-02-22T12:25:17.418424Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_embeddings = model.encode(X_train)\ntest_embeddings = model.encode(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:25:17.420058Z","iopub.execute_input":"2024-02-22T12:25:17.420318Z","iopub.status.idle":"2024-02-22T12:26:43.921763Z","shell.execute_reply.started":"2024-02-22T12:25:17.420295Z","shell.execute_reply":"2024-02-22T12:26:43.920988Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1261 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"016eee70360c42aa9028615eb6963774"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/271 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40812d19b0aa483ea280d9462d396127"}},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n# Initialize KNN classifier\nknn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors as per your requirement\n\n# Train the KNN classifier\nknn_classifier.fit(train_embeddings, Y_train)\n\n# Make predictions on the test data\ny_pred_knn = knn_classifier.predict(test_embeddings)\n\n# Evaluate the performance of the KNN classifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\nprint(\"Accuracy:\", accuracy_score(Y_test, y_pred_knn))\nprint(\"-----------------------------------------------\\n\\n\")\nprint(classification_report(Y_test, y_pred_knn))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:26:43.923013Z","iopub.execute_input":"2024-02-22T12:26:43.923274Z","iopub.status.idle":"2024-02-22T12:26:47.415458Z","shell.execute_reply.started":"2024-02-22T12:26:43.923251Z","shell.execute_reply":"2024-02-22T12:26:47.414497Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Accuracy: 0.6916049953746531\n-----------------------------------------------\n\n\n              precision    recall  f1-score   support\n\n           0       0.65      0.69      0.67        99\n           1       0.76      0.79      0.78       126\n           2       0.61      0.66      0.64       130\n           3       0.80      1.00      0.89        12\n           4       0.49      0.59      0.53       118\n           5       0.24      0.27      0.25        30\n           6       0.47      0.49      0.48       104\n           7       0.45      0.56      0.50         9\n           8       0.50      0.63      0.56       557\n           9       0.25      0.09      0.13        11\n          10       0.60      0.73      0.66       225\n          11       0.00      0.00      0.00         3\n          12       0.75      0.87      0.81       189\n          13       0.47      0.36      0.41        42\n          14       0.00      0.00      0.00         3\n          15       0.60      0.67      0.63         9\n          16       0.00      0.00      0.00         2\n          17       0.75      0.89      0.81        37\n          18       0.00      0.00      0.00         1\n          19       0.00      0.00      0.00         5\n          20       0.74      0.50      0.60        34\n          21       0.10      0.11      0.11         9\n          22       0.50      0.55      0.52        11\n          23       0.68      0.79      0.73       167\n          24       0.00      0.00      0.00         9\n          25       0.49      0.66      0.56        29\n          26       0.54      0.65      0.59        20\n          27       0.42      0.38      0.40       100\n          28       0.79      0.89      0.84       268\n          29       0.78      0.78      0.78        27\n          30       0.25      0.17      0.20        24\n          31       0.51      0.43      0.47       179\n          32       0.41      0.37      0.39        19\n          33       0.53      0.40      0.46       135\n          34       0.66      0.73      0.69       180\n          35       0.68      0.72      0.70        69\n          36       0.00      0.00      0.00         4\n          37       0.59      0.55      0.57        53\n          38       0.58      0.71      0.64        87\n          39       0.17      0.25      0.20         4\n          40       0.77      0.83      0.80       180\n          41       0.79      0.79      0.79       272\n          42       0.73      0.55      0.63        20\n          43       0.76      0.97      0.85        33\n          44       0.33      0.25      0.29         4\n          45       0.00      0.00      0.00         5\n          46       0.44      0.55      0.49        20\n          47       0.33      0.07      0.12        14\n          49       0.62      0.62      0.62        34\n          50       0.55      0.73      0.63        15\n          51       0.51      0.37      0.43       175\n          52       0.72      0.75      0.74        61\n          53       0.00      0.00      0.00         4\n          54       0.81      0.84      0.83       215\n          55       0.67      0.57      0.62         7\n          56       1.00      0.83      0.91         6\n          57       1.00      0.38      0.56        13\n          58       0.00      0.00      0.00        12\n          59       0.54      0.27      0.36        26\n          60       1.00      0.11      0.20         9\n          61       0.67      0.68      0.68        60\n          62       0.70      0.39      0.50        18\n          63       0.72      0.50      0.59        84\n          64       0.42      0.42      0.42        24\n          65       1.00      1.00      1.00         7\n          66       0.87      0.74      0.80        35\n          67       0.54      0.48      0.51        77\n          68       0.68      0.65      0.67       235\n          69       0.50      1.00      0.67         1\n          70       0.70      0.62      0.66       151\n          71       0.67      0.40      0.50        10\n          72       0.50      0.20      0.29         5\n          73       0.75      0.84      0.79        93\n          74       0.80      0.57      0.67        14\n          75       0.92      0.96      0.94        24\n          76       0.71      0.45      0.56        11\n          77       0.90      0.94      0.92        49\n          78       0.50      0.56      0.53         9\n          79       0.00      0.00      0.00         3\n          80       0.80      0.44      0.57        18\n          81       0.00      0.00      0.00         1\n          82       0.88      1.00      0.94        15\n          83       0.50      0.33      0.40         6\n          84       0.76      0.66      0.71       209\n          85       0.74      0.83      0.78       104\n          86       0.64      0.64      0.64        53\n          87       0.00      0.00      0.00         1\n          88       0.95      1.00      0.97        19\n          89       0.70      1.00      0.82         7\n          90       0.71      0.88      0.78        33\n          91       0.78      0.75      0.76       104\n          92       0.00      0.00      0.00         1\n          93       0.53      0.51      0.52       138\n          94       1.00      0.50      0.67         4\n          95       0.82      0.81      0.81       933\n          96       0.60      0.67      0.63         9\n          97       0.45      0.22      0.29        23\n          98       0.81      0.61      0.70        49\n          99       0.00      0.00      0.00         7\n         100       0.80      0.57      0.67        21\n         101       0.50      0.39      0.44        36\n         102       0.12      0.50      0.20         2\n         103       0.79      0.77      0.78        30\n         104       0.75      0.50      0.60        12\n         105       0.22      0.29      0.25         7\n         106       0.00      0.00      0.00         7\n         107       0.86      0.85      0.86       812\n         108       0.85      0.84      0.85        63\n         109       0.64      1.00      0.78         7\n         110       0.75      1.00      0.86         3\n         111       0.73      0.73      0.73        55\n         112       0.77      0.38      0.51        45\n         113       0.62      0.26      0.37        19\n         114       0.57      0.57      0.57         7\n         115       0.77      0.79      0.78        95\n         116       1.00      0.17      0.29         6\n         117       0.67      0.55      0.61        56\n         118       0.76      0.77      0.77       130\n         119       0.68      0.73      0.70       233\n         120       0.64      0.56      0.60        80\n         121       0.00      0.00      0.00         2\n         122       1.00      1.00      1.00        11\n\n    accuracy                           0.69      8648\n   macro avg       0.56      0.52      0.52      8648\nweighted avg       0.69      0.69      0.69      8648\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\nimport joblib\nfrom tqdm import tqdm\n\n# Load the SVM model using joblib\noverclass = joblib.load('/kaggle/input/modelsvm/ovr_classifier.pkl')\n\n# Assuming test_embeddings and Y_test are defined\n# Evaluate the performance of the SVM classifier\n\n# Initialize lists to store predictions\ny_pred_svm = []\n\n# Use tqdm to display progress\nwith tqdm(total=len(test_embeddings), desc=\"Evaluating SVM Classifier\") as pbar:\n    for sample in test_embeddings:\n        # Predict the label for each sample\n        y_pred_svm.append(overclass.predict([sample])[0])\n        # Update progress bar\n        pbar.update(1)\n\n# Convert the list to numpy array\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:34:18.478724Z","iopub.execute_input":"2024-02-22T12:34:18.479425Z","iopub.status.idle":"2024-02-22T12:47:39.448469Z","shell.execute_reply.started":"2024-02-22T12:34:18.479391Z","shell.execute_reply":"2024-02-22T12:47:39.447382Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Evaluating SVM Classifier: 100%|██████████| 8648/8648 [13:20<00:00, 10.80it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Convert the list to numpy array\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m y_pred_svm \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(y_pred_svm)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate accuracy and print classification report\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(Y_test, y_pred_svm))\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"],"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import numpy as np\ny_pred_svm = np.array(y_pred_svm)\n\n# Calculate accuracy and print classification report\nprint(\"Accuracy:\", accuracy_score(Y_test, y_pred_svm))\nprint(\"-----------------------------------------------\\n\\n\")\nprint(classification_report(Y_test, y_pred_svm))","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:48:14.566099Z","iopub.execute_input":"2024-02-22T12:48:14.566464Z","iopub.status.idle":"2024-02-22T12:48:14.599936Z","shell.execute_reply.started":"2024-02-22T12:48:14.566434Z","shell.execute_reply":"2024-02-22T12:48:14.599039Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Accuracy: 0.72895467160037\n-----------------------------------------------\n\n\n              precision    recall  f1-score   support\n\n           0       0.75      0.70      0.72        99\n           1       0.81      0.79      0.80       126\n           2       0.65      0.70      0.67       130\n           3       0.86      1.00      0.92        12\n           4       0.67      0.57      0.61       118\n           5       0.30      0.10      0.15        30\n           6       0.65      0.53      0.58       104\n           7       0.83      0.56      0.67         9\n           8       0.58      0.66      0.62       557\n           9       0.50      0.09      0.15        11\n          10       0.71      0.69      0.70       225\n          11       0.00      0.00      0.00         3\n          12       0.77      0.92      0.84       189\n          13       0.81      0.40      0.54        42\n          14       0.00      0.00      0.00         3\n          15       0.67      0.67      0.67         9\n          16       0.00      0.00      0.00         2\n          17       0.85      0.89      0.87        37\n          18       0.00      0.00      0.00         1\n          19       0.00      0.00      0.00         5\n          20       0.81      0.50      0.62        34\n          21       0.00      0.00      0.00         9\n          22       0.67      0.55      0.60        11\n          23       0.72      0.91      0.80       167\n          24       0.00      0.00      0.00         9\n          25       0.86      0.62      0.72        29\n          26       0.79      0.75      0.77        20\n          27       0.59      0.33      0.42       100\n          28       0.81      0.93      0.87       268\n          29       0.90      0.67      0.77        27\n          30       0.33      0.21      0.26        24\n          31       0.60      0.43      0.50       179\n          32       0.33      0.11      0.16        19\n          33       0.65      0.41      0.50       135\n          34       0.70      0.78      0.73       180\n          35       0.74      0.88      0.81        69\n          36       0.00      0.00      0.00         4\n          37       0.61      0.53      0.57        53\n          38       0.59      0.69      0.64        87\n          39       0.00      0.00      0.00         4\n          40       0.79      0.83      0.81       180\n          41       0.81      0.80      0.80       272\n          42       0.92      0.60      0.73        20\n          43       0.86      0.97      0.91        33\n          44       0.00      0.00      0.00         4\n          45       0.00      0.00      0.00         5\n          46       0.48      0.60      0.53        20\n          47       0.75      0.21      0.33        14\n          49       0.62      0.47      0.53        34\n          50       0.79      0.73      0.76        15\n          51       0.70      0.58      0.63       175\n          52       0.71      0.87      0.78        61\n          53       0.50      0.25      0.33         4\n          54       0.80      0.87      0.83       215\n          55       0.83      0.71      0.77         7\n          56       1.00      0.83      0.91         6\n          57       0.75      0.46      0.57        13\n          58       0.50      0.08      0.14        12\n          59       0.55      0.42      0.48        26\n          60       0.67      0.22      0.33         9\n          61       0.81      0.70      0.75        60\n          62       0.67      0.44      0.53        18\n          63       0.68      0.68      0.68        84\n          64       0.65      0.54      0.59        24\n          65       0.88      1.00      0.93         7\n          66       0.74      0.71      0.72        35\n          67       0.57      0.65      0.61        77\n          68       0.68      0.71      0.69       235\n          69       0.00      0.00      0.00         1\n          70       0.66      0.66      0.66       151\n          71       0.50      0.30      0.37        10\n          72       1.00      0.40      0.57         5\n          73       0.77      0.89      0.83        93\n          74       0.36      0.29      0.32        14\n          75       0.96      0.96      0.96        24\n          76       1.00      0.45      0.62        11\n          77       0.92      0.92      0.92        49\n          78       0.54      0.78      0.64         9\n          79       0.00      0.00      0.00         3\n          80       0.90      0.50      0.64        18\n          81       0.00      0.00      0.00         1\n          82       0.78      0.93      0.85        15\n          83       0.50      0.33      0.40         6\n          84       0.75      0.65      0.70       209\n          85       0.75      0.85      0.79       104\n          86       0.63      0.75      0.69        53\n          87       0.00      0.00      0.00         1\n          88       0.86      1.00      0.93        19\n          89       1.00      1.00      1.00         7\n          90       0.66      0.88      0.75        33\n          91       0.76      0.77      0.77       104\n          92       0.00      0.00      0.00         1\n          93       0.58      0.52      0.55       138\n          94       1.00      0.25      0.40         4\n          95       0.79      0.85      0.82       933\n          96       0.54      0.78      0.64         9\n          97       0.50      0.39      0.44        23\n          98       0.82      0.76      0.79        49\n          99       0.00      0.00      0.00         7\n         100       0.86      0.57      0.69        21\n         101       0.59      0.44      0.51        36\n         102       0.25      0.50      0.33         2\n         103       0.73      0.80      0.76        30\n         104       0.82      0.75      0.78        12\n         105       0.24      0.57      0.33         7\n         106       0.00      0.00      0.00         7\n         107       0.85      0.90      0.88       812\n         108       0.80      0.89      0.84        63\n         109       0.86      0.86      0.86         7\n         110       0.50      1.00      0.67         3\n         111       0.84      0.78      0.81        55\n         112       0.73      0.53      0.62        45\n         113       0.57      0.21      0.31        19\n         114       0.83      0.71      0.77         7\n         115       0.75      0.89      0.81        95\n         116       0.00      0.00      0.00         6\n         117       0.60      0.82      0.69        56\n         118       0.71      0.80      0.75       130\n         119       0.69      0.71      0.70       233\n         120       0.61      0.66      0.63        80\n         121       0.00      0.00      0.00         2\n         122       1.00      1.00      1.00        11\n\n    accuracy                           0.73      8648\n   macro avg       0.59      0.54      0.55      8648\nweighted avg       0.72      0.73      0.72      8648\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Initialize KNN classifier\nknn_classifier = KNeighborsClassifier(n_neighbors=12)\n\n# Initialize BaggingClassifier with KNN as the base estimator\nbagging_classifier = BaggingClassifier(base_estimator=knn_classifier, n_estimators=10, random_state=42)\n\n# Train the BaggingClassifier\nbagging_classifier.fit(train_embeddings, Y_train)\n\n# Make predictions on the test data\ny_pred_bagging = bagging_classifier.predict(test_embeddings)\n\n# Evaluate the performance of the BaggingClassifier with KNN\nprint(\"Bagging Classifier Performance:\")\nprint(\"Accuracy:\", accuracy_score(Y_test, y_pred_bagging))\nprint(classification_report(Y_test, y_pred_bagging))\nprint(\"-----------------------------------------------\\n\\n\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T12:56:53.581996Z","iopub.execute_input":"2024-02-22T12:56:53.582722Z","iopub.status.idle":"2024-02-22T12:57:24.551277Z","shell.execute_reply.started":"2024-02-22T12:56:53.582692Z","shell.execute_reply":"2024-02-22T12:57:24.550331Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Bagging Classifier Performance:\nAccuracy: 0.7035152636447733\n              precision    recall  f1-score   support\n\n           0       0.73      0.68      0.70        99\n           1       0.79      0.80      0.80       126\n           2       0.66      0.62      0.64       130\n           3       0.86      1.00      0.92        12\n           4       0.60      0.52      0.55       118\n           5       0.40      0.07      0.11        30\n           6       0.57      0.45      0.50       104\n           7       0.75      0.33      0.46         9\n           8       0.53      0.61      0.56       557\n           9       0.00      0.00      0.00        11\n          10       0.65      0.72      0.68       225\n          11       1.00      0.33      0.50         3\n          12       0.75      0.88      0.81       189\n          13       0.65      0.36      0.46        42\n          14       0.00      0.00      0.00         3\n          15       0.75      0.67      0.71         9\n          16       0.00      0.00      0.00         2\n          17       0.82      0.86      0.84        37\n          18       0.00      0.00      0.00         1\n          19       0.00      0.00      0.00         5\n          20       0.80      0.47      0.59        34\n          21       0.25      0.11      0.15         9\n          22       0.75      0.55      0.63        11\n          23       0.70      0.88      0.78       167\n          24       0.00      0.00      0.00         9\n          25       0.64      0.62      0.63        29\n          26       0.52      0.65      0.58        20\n          27       0.54      0.32      0.40       100\n          28       0.78      0.94      0.85       268\n          29       0.76      0.70      0.73        27\n          30       0.18      0.08      0.11        24\n          31       0.54      0.46      0.50       179\n          32       0.50      0.21      0.30        19\n          33       0.59      0.36      0.44       135\n          34       0.67      0.76      0.72       180\n          35       0.70      0.81      0.75        69\n          36       0.00      0.00      0.00         4\n          37       0.50      0.62      0.55        53\n          38       0.57      0.69      0.62        87\n          39       0.00      0.00      0.00         4\n          40       0.75      0.86      0.80       180\n          41       0.82      0.81      0.82       272\n          42       0.92      0.55      0.69        20\n          43       0.79      0.94      0.86        33\n          44       0.00      0.00      0.00         4\n          45       0.00      0.00      0.00         5\n          46       0.44      0.55      0.49        20\n          47       0.50      0.14      0.22        14\n          49       0.58      0.41      0.48        34\n          50       0.50      0.73      0.59        15\n          51       0.53      0.46      0.49       175\n          52       0.74      0.84      0.78        61\n          53       0.20      0.25      0.22         4\n          54       0.82      0.85      0.83       215\n          55       0.75      0.43      0.55         7\n          56       1.00      0.67      0.80         6\n          57       1.00      0.31      0.47        13\n          58       0.00      0.00      0.00        12\n          59       0.42      0.19      0.26        26\n          60       1.00      0.22      0.36         9\n          61       0.66      0.65      0.66        60\n          62       0.67      0.33      0.44        18\n          63       0.73      0.52      0.61        84\n          64       0.50      0.46      0.48        24\n          65       0.88      1.00      0.93         7\n          66       0.81      0.71      0.76        35\n          67       0.55      0.56      0.55        77\n          68       0.68      0.64      0.66       235\n          69       0.00      0.00      0.00         1\n          70       0.66      0.64      0.65       151\n          71       0.25      0.10      0.14        10\n          72       1.00      0.20      0.33         5\n          73       0.76      0.87      0.81        93\n          74       0.67      0.29      0.40        14\n          75       0.85      0.92      0.88        24\n          76       1.00      0.36      0.53        11\n          77       0.83      0.90      0.86        49\n          78       0.42      0.56      0.48         9\n          79       0.00      0.00      0.00         3\n          80       0.80      0.44      0.57        18\n          81       0.00      0.00      0.00         1\n          82       0.80      0.80      0.80        15\n          83       1.00      0.17      0.29         6\n          84       0.75      0.61      0.67       209\n          85       0.67      0.85      0.75       104\n          86       0.68      0.74      0.71        53\n          87       0.00      0.00      0.00         1\n          88       0.76      1.00      0.86        19\n          89       0.78      1.00      0.88         7\n          90       0.60      0.88      0.72        33\n          91       0.76      0.74      0.75       104\n          92       0.00      0.00      0.00         1\n          93       0.53      0.47      0.50       138\n          94       0.00      0.00      0.00         4\n          95       0.80      0.85      0.82       933\n          96       0.55      0.67      0.60         9\n          97       0.42      0.22      0.29        23\n          98       0.80      0.73      0.77        49\n          99       0.00      0.00      0.00         7\n         100       0.87      0.62      0.72        21\n         101       0.56      0.42      0.48        36\n         102       0.00      0.00      0.00         2\n         103       0.75      0.80      0.77        30\n         104       0.50      0.42      0.45        12\n         105       0.21      0.43      0.29         7\n         106       0.00      0.00      0.00         7\n         107       0.85      0.88      0.86       812\n         108       0.82      0.89      0.85        63\n         109       0.58      1.00      0.74         7\n         110       0.60      1.00      0.75         3\n         111       0.76      0.82      0.79        55\n         112       0.70      0.31      0.43        45\n         113       0.40      0.21      0.28        19\n         114       0.80      0.57      0.67         7\n         115       0.75      0.82      0.78        95\n         116       0.00      0.00      0.00         6\n         117       0.62      0.70      0.66        56\n         118       0.72      0.81      0.76       130\n         119       0.65      0.79      0.71       233\n         120       0.63      0.65      0.64        80\n         121       0.00      0.00      0.00         2\n         122       0.85      1.00      0.92        11\n\n    accuracy                           0.70      8648\n   macro avg       0.55      0.49      0.50      8648\nweighted avg       0.69      0.70      0.69      8648\n\n-----------------------------------------------\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n# Define the parameter grid to search\nparam_grid = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization strength\n    'max_iter': [100, 500, 1000]  # Maximum number of iterations\n}\n\n# Initialize Logistic Regression classifier\nlogistic_regression_classifier = LogisticRegression()\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(logistic_regression_classifier, param_grid, cv=5, scoring='accuracy')\n\n# Perform grid search to find the best parameters\ngrid_search.fit(train_embeddings, Y_train)\n\n# Get the best estimator\nbest_logistic_regression_classifier = grid_search.best_estimator_\n\n# Make predictions on the test data\ny_pred_best_logistic_regression = best_logistic_regression_classifier.predict(test_embeddings)\n\n# Evaluate the performance of the Logistic Regression classifier with the best parameters\nprint(\"Best Logistic Regression Classifier Performance:\")\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Accuracy:\", accuracy_score(Y_test, y_pred_best_logistic_regression))\nprint(classification_report(Y_test, y_pred_best_logistic_regression))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T13:15:19.669186Z","iopub.execute_input":"2024-02-22T13:15:19.669873Z","iopub.status.idle":"2024-02-22T14:34:38.523831Z","shell.execute_reply.started":"2024-02-22T13:15:19.669839Z","shell.execute_reply":"2024-02-22T14:34:38.522859Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Best Logistic Regression Classifier Performance:\nBest Parameters: {'C': 10, 'max_iter': 500}\nAccuracy: 0.6763413506012951\n              precision    recall  f1-score   support\n\n           0       0.70      0.65      0.67        99\n           1       0.77      0.79      0.78       126\n           2       0.63      0.66      0.65       130\n           3       0.79      0.92      0.85        12\n           4       0.55      0.48      0.51       118\n           5       0.40      0.27      0.32        30\n           6       0.45      0.38      0.41       104\n           7       0.40      0.44      0.42         9\n           8       0.55      0.62      0.59       557\n           9       0.25      0.18      0.21        11\n          10       0.64      0.65      0.64       225\n          11       0.00      0.00      0.00         3\n          12       0.72      0.82      0.77       189\n          13       0.64      0.33      0.44        42\n          14       0.00      0.00      0.00         3\n          15       0.43      0.33      0.38         9\n          16       0.00      0.00      0.00         2\n          17       0.68      0.76      0.72        37\n          18       0.00      0.00      0.00         1\n          19       0.00      0.00      0.00         5\n          20       0.62      0.38      0.47        34\n          21       0.33      0.11      0.17         9\n          22       0.70      0.64      0.67        11\n          23       0.70      0.84      0.77       167\n          24       0.00      0.00      0.00         9\n          25       0.52      0.52      0.52        29\n          26       0.80      0.60      0.69        20\n          27       0.33      0.23      0.27       100\n          28       0.82      0.89      0.85       268\n          29       0.72      0.67      0.69        27\n          30       0.33      0.21      0.26        24\n          31       0.44      0.40      0.42       179\n          32       0.38      0.26      0.31        19\n          33       0.65      0.40      0.50       135\n          34       0.66      0.70      0.68       180\n          35       0.81      0.83      0.82        69\n          36       0.00      0.00      0.00         4\n          37       0.52      0.47      0.50        53\n          38       0.55      0.68      0.61        87\n          39       0.29      0.50      0.36         4\n          40       0.79      0.81      0.80       180\n          41       0.75      0.74      0.75       272\n          42       0.83      0.50      0.62        20\n          43       0.82      0.94      0.87        33\n          44       0.00      0.00      0.00         4\n          45       0.00      0.00      0.00         5\n          46       0.44      0.70      0.54        20\n          47       0.67      0.14      0.24        14\n          49       0.34      0.32      0.33        34\n          50       0.71      0.67      0.69        15\n          51       0.62      0.51      0.56       175\n          52       0.69      0.77      0.73        61\n          53       0.25      0.25      0.25         4\n          54       0.74      0.80      0.77       215\n          55       0.83      0.71      0.77         7\n          56       1.00      0.67      0.80         6\n          57       1.00      0.46      0.63        13\n          58       0.33      0.08      0.13        12\n          59       0.55      0.62      0.58        26\n          60       0.33      0.11      0.17         9\n          61       0.62      0.65      0.63        60\n          62       0.67      0.44      0.53        18\n          63       0.59      0.62      0.60        84\n          64       0.50      0.38      0.43        24\n          65       1.00      0.71      0.83         7\n          66       0.77      0.77      0.77        35\n          67       0.50      0.58      0.54        77\n          68       0.62      0.63      0.62       235\n          69       0.00      0.00      0.00         1\n          70       0.64      0.58      0.61       151\n          71       0.50      0.40      0.44        10\n          72       0.00      0.00      0.00         5\n          73       0.81      0.85      0.83        93\n          74       0.29      0.29      0.29        14\n          75       0.85      0.92      0.88        24\n          76       0.83      0.45      0.59        11\n          77       0.96      0.92      0.94        49\n          78       0.33      0.44      0.38         9\n          79       0.00      0.00      0.00         3\n          80       0.69      0.50      0.58        18\n          81       0.00      0.00      0.00         1\n          82       0.76      0.87      0.81        15\n          83       0.33      0.33      0.33         6\n          84       0.59      0.58      0.58       209\n          85       0.71      0.77      0.74       104\n          86       0.60      0.64      0.62        53\n          87       0.00      0.00      0.00         1\n          88       0.90      0.95      0.92        19\n          89       1.00      1.00      1.00         7\n          90       0.77      0.82      0.79        33\n          91       0.73      0.74      0.73       104\n          92       0.00      0.00      0.00         1\n          93       0.54      0.54      0.54       138\n          94       0.00      0.00      0.00         4\n          95       0.75      0.79      0.77       933\n          96       0.62      0.89      0.73         9\n          97       0.33      0.30      0.32        23\n          98       0.70      0.67      0.69        49\n          99       0.00      0.00      0.00         7\n         100       0.71      0.57      0.63        21\n         101       0.61      0.47      0.53        36\n         102       0.33      0.50      0.40         2\n         103       0.80      0.80      0.80        30\n         104       1.00      0.17      0.29        12\n         105       0.27      0.43      0.33         7\n         106       0.33      0.14      0.20         7\n         107       0.82      0.84      0.83       812\n         108       0.85      0.89      0.87        63\n         109       1.00      1.00      1.00         7\n         110       0.40      0.67      0.50         3\n         111       0.79      0.80      0.79        55\n         112       0.48      0.51      0.49        45\n         113       0.25      0.11      0.15        19\n         114       0.33      0.29      0.31         7\n         115       0.73      0.84      0.78        95\n         116       0.00      0.00      0.00         6\n         117       0.62      0.71      0.67        56\n         118       0.71      0.78      0.74       130\n         119       0.69      0.70      0.69       233\n         120       0.53      0.60      0.56        80\n         121       0.00      0.00      0.00         2\n         122       1.00      0.91      0.95        11\n\n    accuracy                           0.68      8648\n   macro avg       0.53      0.49      0.50      8648\nweighted avg       0.67      0.68      0.67      8648\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# Assuming you already have best_logistic_regression_classifier, bagging_classifier, and overclass models\n\n# Create a list of tuples containing (name, model) pairs\nestimators = [('Logistic Regression', best_logistic_regression_classifier),\n              ('Bagging Classifier', bagging_classifier),\n              ('Overclass', overclass)]\n\n# Initialize the VotingClassifier with the estimators\nvoting_classifier = VotingClassifier(estimators, voting='hard')  # You can use 'soft' voting if models provide predict_proba\n\n# Train the VotingClassifier\nvoting_classifier.fit(train_embeddings, Y_train)\n\n# Make predictions on the test data\ny_pred_voting = voting_classifier.predict(test_embeddings)\n\n# Evaluate the performance of the VotingClassifier\nprint(\"Voting Classifier Performance:\")\nprint(\"Accuracy:\", accuracy_score(Y_test, y_pred_voting))\nprint(classification_report(Y_test, y_pred_voting))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T14:59:17.769279Z","iopub.execute_input":"2024-02-22T14:59:17.770180Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}